{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88322f26-1bc8-4ea9-a4a4-e69a4b0ff061",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e9ad03b-508f-4287-83c4-fc115747aea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b301923-9754-4355-9e54-1d98e3ff7d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_embeddings_from_folder(folder_path, output_json=\"embeddings.json\"):\n",
    "    model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "    data = []\n",
    "\n",
    "    for filename in sorted(os.listdir(folder_path)):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            node_id = filename\n",
    "            with open(os.path.join(folder_path, filename), \"r\", encoding=\"utf-8\") as f:\n",
    "                text = f.read().strip()\n",
    "                if not text:\n",
    "                    continue  # Skip empty files\n",
    "                embedding = model.encode(text).tolist()\n",
    "                data.append({\n",
    "                    \"node_id\": node_id,\n",
    "                    \"text\": text,\n",
    "                    \"embedding\": embedding\n",
    "                })\n",
    "\n",
    "    with open(output_json, \"w\", encoding=\"utf-8\") as out_f:\n",
    "        json.dump(data, out_f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"✅ Embedded {len(data)} texts and saved to {output_json}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83ebf54c-e8b8-47c2-97de-2ded358bb429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Embedded 103 texts and saved to mistici_hoti_nebuni_nodes_embeddings.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "generate_embeddings_from_folder(\"mistici_hoti_nebuni\", \"mistici_hoti_nebuni_node_embeddings.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9d2edb6-56ad-4e2c-ace7-321fbec1d059",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wildcard = \"Diane Rothenberg [NOT] feeling left out of a Paradise of Poets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "297e82da-4653-43d8-a945-e42dbc96d6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wildcard_embedding = model.encode(wildcard, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "426bb0bf-9989-4b85-a4dd-a9c1653f60cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def build_multidigraph_from_embeddings(json_path, wildcard_embedding, threshold_1=0.50, threshold_2=0.75):\n",
    "    # Load node data\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        nodes = json.load(f)\n",
    "\n",
    "    G = nx.MultiDiGraph()\n",
    "\n",
    "    # Prepare embeddings as array\n",
    "    node_ids = [node[\"node_id\"] for node in nodes]\n",
    "    embeddings = np.array([node[\"embedding\"] for node in nodes])\n",
    "    wildcard_emb = np.array(wildcard_embedding).reshape(1, -1)\n",
    "\n",
    "    # Compute cosine similarity matrix\n",
    "    sim_matrix = cosine_similarity(embeddings)\n",
    "    wildcard_sims = cosine_similarity(embeddings, wildcard_emb).flatten()\n",
    "\n",
    "    # Add nodes\n",
    "    for i, node in enumerate(nodes):\n",
    "        G.add_node(node[\"node_id\"], text=node[\"text\"], embedding=node[\"embedding\"], wildcard_sim=wildcard_sims[i])\n",
    "\n",
    "    # --- Type 1 Edges: Similar pairs above threshold_1 ---\n",
    "    for i in range(len(nodes)):\n",
    "        for j in range(len(nodes)):\n",
    "            if i != j and sim_matrix[i][j] >= threshold_1:\n",
    "                # Decide edge source: more similar to wildcard\n",
    "                source = node_ids[i] if wildcard_sims[i] >= wildcard_sims[j] else node_ids[j]\n",
    "                target = node_ids[j] if source == node_ids[i] else node_ids[i]\n",
    "                G.add_edge(source, target, weight=1, type=\"embedding_sim\")\n",
    "\n",
    "    # --- Type 2 Edges: Both nodes must be similar to wildcard ---\n",
    "    wildcard_above = [i for i, sim in enumerate(wildcard_sims) if sim >= threshold_2]\n",
    "    for i in wildcard_above:\n",
    "        for j in wildcard_above:\n",
    "            if i < j:\n",
    "                source = node_ids[i] if wildcard_sims[i] >= wildcard_sims[j] else node_ids[j]\n",
    "                target = node_ids[j] if source == node_ids[i] else node_ids[i]\n",
    "                G.add_edge(source, target, weight=2, type=\"wildcard_cluster\")\n",
    "\n",
    "    print(f\"✅ Graph created with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges.\")\n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40f7f7f7-d81b-4f44-80d2-1f0a0d430b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wildcard_embedding = model.encode(wildcard, convert_to_numpy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a03fbb37-2d01-49be-a844-4050fcca31fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Graph created with 103 nodes and 1102 edges.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "jerry_mistici_hoti_nebuni_plus_wildcard_graph = build_multidigraph_from_embeddings(\"mistici_hoti_nebuni_node_embeddings.json\", wildcard_embedding=wildcard_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a69b613f-7e32-4c9a-aa9b-d2630358affb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Graph created with 103 nodes and 1117 edges.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "jerry_mistici_hoti_nebuni_plus_wildcard_graph_1 = \\\n",
    "build_multidigraph_from_embeddings(\"mistici_hoti_nebuni_node_embeddings.json\", wildcard_embedding=wildcard_embedding, threshold_1=0.50, threshold_2=0.50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a42b9d8-4fd6-4b4e-8df8-dde76d36a3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61424331-201c-4eac-849d-d71ad9ef7f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('margento_jerry_mistici_hoti_nebuni_plus_wildcard_graph.gpickle', 'wb') as f:\n",
    "    pickle.dump(jerry_mistici_hoti_nebuni_plus_wildcard_graph_1, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d1c557-3605-4502-a240-1a2df75c7f55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (3918project)",
   "language": "python",
   "name": "3918project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
